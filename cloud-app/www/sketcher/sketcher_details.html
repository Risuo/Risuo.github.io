<!doctype html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8"/>
    <title>Peter Scott Miller</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">

    <!-- Option 1: Bootstrap Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
            crossorigin="anonymous"></script>

</head>

<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container-xl my-md-4">
        <a class="navbar-brand" href="../index.html"><h1>Peter Scott Miller</h1></a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                <li class="nav-item">
                    <a class="nav-link active" aria-current="page" href="../index.html">Home</a>
                </li>

                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button"
                       data-bs-toggle="dropdown" aria-expanded="false">
                        Navigate
                    </a>
                    <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                        <li><a class="dropdown-item" href="../index.html">Home</a></li>
                        <li><a class="dropdown-item" href="../building_identification/index.html">Building
                            Identification</a></li>
                        <li><a class="dropdown-item" href="./index.html">Quick-Sketch</a></li>
                        <li>
                            <hr class="dropdown-divider">
                        </li>
                        <li><a class="dropdown-item" href="../building_identification/building_details.html">Building
                            Identification - Details</a></li>
                        <li><a class="dropdown-item" href="./sketcher_details.html">Quick-Sketch - Details</a>
                        </li>
                    </ul>
                </li>
            </ul>
        </div>
    </div>
</nav>

<!-- bs4 css-->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.1.0/css/bootstrap.min.css"/>

<!-- main css -->
<link rel="stylesheet" href="./homepage/css/main.css">
<div class="container-xl my-md-4">
    <div class="portfolio">

        <div class="accordion-item">

            <h2 class="accordion-header" id="headingOne">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                        data-bs-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                    Initial training & basic setup.
                </button>
            </h2>
            <div id="collapseOne" class="accordion-collapse collapse" aria-labelledby="headingOne"
                 data-bs-parent="#accordionExample">

                <div class="accordion-body">
                    <am class="row">
                        <li> This project was inspired by a smaller Sketcher application developed by <a
                                href='https://zaidalyafeai.github.io/' target="_blank"
                                title="Zaid Alyafeai"> Zaid Alyafeai</a>.
                        <li> There are two models present on this page. Both were trained on the full <a
                                href='https://github.com/googlecreativelab/quickdraw-dataset'
                                target="_blank"> 345 class dataset </a> from Google.
                        </li>
                        <li>The link to an earlier, simpler* Colaboratory Notebook prototype with only 160 classes
                            is available here: <a
                                    href='https://colab.research.google.com/github/Risuo/Springboard/blob/master/Revised_TPU_Quick_Draw_Data.ipynb'
                                    target="_blank"> Colab</a>
                            <p><small> *This smaller prototype was trained using Google's Tensor Processing Unit
                                (TPU) </small>
                            </p>
                        </li>
                        <li>
                            The architecture of training is defined by the following Convolutional Neural Network
                            structure:
                            <p></p>
                            <p><img src="images/CNN_Model.PNG?raw=true" alt="CNN_Model.PNG"></p>
                            <p></p>
                        </li>
                        <li>
                            The specific architecture was arrived at through 50 rounds of optimization, performed by
                            scikit-optimize, selecting from the following parameters:
                        </li>
                        <p></p>
                        <p><img src="images/Optimize_Parameters.PNG?raw=true" alt="Optimize_Parameter.PNG"></p>
                        <p></p>
                        <li>The optimization was run on a subset of the full dataset, specifically only 60,000 training
                            and 5,000 validation sets.
                        </li>
                        <li>Here is the convergence plot of the optimization:</li>
                        <p></p>
                        <p><img src="images/Convergence_plot.PNG?raw=true" alt="Convergence_plot.PNG"></p>
                        <li>The full dataset contained 1,656,000 sets, with an 80/20 train/validation ratio.</li>
                        <p></p>
                        <p><img src="images/training.PNG?raw=true" alt="training.PNG"></p>
                        <li>Below are two charts, showing the accuracy and loss, respectively, over the training
                            epochs:
                        </li>
                        <p></p>
                        <p><img src="images/Accuracy_over_epoch.PNG?raw=true" alt="Accuracy_over_epoch.PNG">
                            <img src="images/Loss_over_epoch.PNG?raw=true" alt="Loss_over_epoch.PNG"></p>
                        <li>Minor overfitting is present after roughly the 18th epoch.</li>
                    </am>
                </div>
            </div>

        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingTwo">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                        data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                    Differences between the 28x28 and 52x52 models:
                </button>
            </h2>
            <div id="collapseTwo" class="accordion-collapse collapse" aria-labelledby="headingTwo"
                 data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <!-- Inner content for the second accordion item can be added here -->
                    <li> The 28x28 pixel model was the first one trained. When reviewing the performance, it was found
                        that the model had difficulty differentiating between visually similar classes.
                    </li>
                    <li>For example, the model had a difficult time successfully differentiating between the following
                        class groups: [circle, stop sign, octagon, hexagon] or [house, barn] or [butterfly, bowtie,
                        ant].
                    </li>
                    <li> The raw dataset contained 256x256 images, and so my theory was that performing a less
                        significant downsampling would result in a higher accuracy within (at least) these similar
                        groups.
                    </li>
                    <li>
                        However, increasing the training dataset size by a factor of four (784 pixels to 3,136) led to a
                        number
                        of challenges based on hardware limitations used in training.
                    </li>
                    <li>
                        The training setup was via Google Colab, chosen for price, accessibility, and ease of CUDA
                        integration.
                        However, I was limited to the hardware Google provided, which included only 15GB of GPU RAM.
                    </li>
                    <li>
                        This was a sufficient amount of RAM to load the entire 1.6 Million training samples when they
                        were
                        only 784 pixels each, but unfortunately required that I split the dataset up into 3 batches once
                        they
                        were 3,136 pixels each.
                    </li>
                    <li>
                        The training itself was run via the following training scheme:
                    </li>
                    <p></p>
                    <p><img src="images/model_fit.PNG?raw=true" alt="training.PNG"></p>
                    <li>
                        <p></p>
                        <footer class="blockquote-footer">Peter Miller</footer>
                    </li>

                </div>
            </div>
        </div>
    </div>
</div>


